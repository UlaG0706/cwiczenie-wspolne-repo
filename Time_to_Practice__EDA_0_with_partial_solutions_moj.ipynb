{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UlaG0706/cwiczenie-wspolne-repo/blob/main/Time_to_Practice__EDA_0_with_partial_solutions_moj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dJqdR9YtWDy",
      "metadata": {
        "id": "6dJqdR9YtWDy"
      },
      "source": [
        "<img height=\"180px\" src=\"https://drive.google.com/uc?export=view&id=141XOz6N4nk8Ru1sAl7vOsAToCLrSFCAX\" alt=\"SDA logo\" align=\"left\" hspace=\"30px\" vspace=\"50px\"/>\n",
        "\n",
        "# Welcome to your next notebook with SDA!\n",
        "\n",
        "During the classes we will mostly use [Google Colaboratory](https://colab.research.google.com/?hl=en) which is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
        "\n",
        "However, for bigger projects, especially involving Deep Learning and/or big data reading, it might be a better choice to setup Jupyter Notebook or Jupyter Lab on your computer. Also, it is worth noticing that there is a great number of useful extensions (see [nbextensions](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/index.html) and [jupyter-labextension](https://jupyterlab.readthedocs.io/en/stable/user/extensions.html)) not available for Colab users."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "keFJAWlchkBg",
      "metadata": {
        "id": "keFJAWlchkBg"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1UO2urRciECzoKE_vHy4RMGfFbkOWOGlW\" alt=\"SDA logo\" align=\"left\" width=\"100px\" hspace=\"10px\" vspace=\"10px\"/>\n",
        "\n",
        "# Exploratory Data Analysis\n",
        "## Data Cleaning and Preparation\n",
        "\n",
        "Data understanding - collecting specific data needed to perform the following tasks:\n",
        "* solve the problem\n",
        "* provide data description (sometimes also labeling),\n",
        "* exploration and quality verification.\n",
        "\n",
        "Data types - `.csv` sheets (these are the ones we will be working on today), text, image, sound, video, weather data, stock data, public archives, etc.\n",
        "\n",
        "The most important task - FEATURE EXTRACTION (feature engineering). A lot depends on the size of the set and the number of features - depending on these parameters, we will take different steps in the data analysis process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WLygrpRfPAvG",
      "metadata": {
        "id": "WLygrpRfPAvG"
      },
      "source": [
        "#### **<font color='#306998'>Download </font><font color='#ffd33b'>chipotle.csv</font>**\n",
        "\n",
        "Download `chipotle.tsv` file from the following kaggle site:\n",
        "https://www.kaggle.com/datasets/navneethc/chipotle\n",
        "\n",
        "Upload it to your Google Drive, mount it and load the data.\n",
        "\n",
        "*Hint: you can copy the path to the file or change the working directory and then use just the file name with .tsv extenstion.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Kqd6M4ODzMdU"
      },
      "id": "Kqd6M4ODzMdU",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/matzim95/ML-datasets/master/chipotle.tsv'"
      ],
      "metadata": {
        "id": "R-Azn7pszoYh"
      },
      "id": "R-Azn7pszoYh",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "RDQpa66cPnNq",
      "metadata": {
        "id": "RDQpa66cPnNq"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "BlMmiCELqv53"
      },
      "id": "BlMmiCELqv53",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ul0Wp-dyQhK5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ul0Wp-dyQhK5",
        "outputId": "421e03a4-2120-42ae-c8e4-d8a2e9f7d8a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/SDA Data Science/files/chipotle.tsv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-782433b09383>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# nie chce sie laczyc z dyskiem dlatego nie wywoluje tego\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchipotle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/SDA Data Science/files/chipotle.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/SDA Data Science/files/chipotle.tsv'"
          ]
        }
      ],
      "source": [
        "# nie chce sie laczyc z dyskiem dlatego nie wywoluje tego\n",
        "chipotle = pd.read_csv(\"/content/drive/MyDrive/SDA Data Science/files/chipotle.tsv\", sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle['item_price [$]'] = chipotle[\"item_price\"].str.replace(\"$\", \"\", regex=False).astype(float)\n",
        "chipotle = chipotle.drop(columns=[\"item_price\"])\n",
        "chipotle.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "FxKB8cWNrWFn",
        "outputId": "207d4651-e444-4f4e-c8e2-6f697eba7c32"
      },
      "id": "FxKB8cWNrWFn",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chipotle' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d4bda3055e13>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchipotle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_price [$]'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchipotle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"item_price\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"$\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchipotle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchipotle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"item_price\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchipotle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chipotle' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "0vqYwyaO0qdL",
        "outputId": "96606c41-c6bf-4209-8711-1cd6efd53711"
      },
      "id": "0vqYwyaO0qdL",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chipotle' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4813515b3f5d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchipotle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'chipotle' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U6xwPMKRgXZC",
      "metadata": {
        "id": "U6xwPMKRgXZC"
      },
      "source": [
        "### Dealing with missing data\n",
        "\n",
        "Missing data is a fairly common problem faced by Data Analysts and Data Scientist. There are two main approaches:\n",
        "* `df.dropna()` method - removes all rows with empty data from the table\n",
        "* `df.fillna(value)` method- fills (or imputes) the data with the specified value\n",
        "\n",
        "Later on you will also know some more advanced methods to impute missing data. What are your thoughts? What number should be given to the `<value>` to fill in missing observations?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chipotle) #sprawdzam ile wgl mam obserwacji"
      ],
      "metadata": {
        "id": "g9GOfwbH00gH"
      },
      "id": "g9GOfwbH00gH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w-kKKMLhgW8z",
      "metadata": {
        "id": "w-kKKMLhgW8z"
      },
      "outputs": [],
      "source": [
        "chipotle.isna().sum() # tu sprawdzamy ile braków jest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jY9wuWbihUS0",
      "metadata": {
        "id": "jY9wuWbihUS0"
      },
      "outputs": [],
      "source": [
        "chipotle['choice_description'].fillna(\"No description\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qds8keU8kRuK",
      "metadata": {
        "id": "qds8keU8kRuK"
      },
      "source": [
        "Incomplete values - how to deal with them:\n",
        "* ignore them #tego sie raczej nie uzywa\n",
        "* insert the value \"unknown\" #oznaczenie porpostu ze to brak danych -> np mamy mieszkania i pietra i mamy mieszkania gdzie nie mamy przypisanego pietra. za pomoca jakiej wartosci oznaczymy bez zmieniania typu danych ze pietro nie ostnieje. mozna wziac np liczbe bardzo wysoka 99, (o ile nie sa to mieszkania w dubaju :P , albo -1 np, 0 nie bo sa mieszkania na parterze. Chodzi o to zeby dac taka wartosc ktora nie wystepuje w danych)\n",
        "* manually fill in on the basis of accumulated knowledge\n",
        "* delete records with incomplete data\n",
        "* complete algorithmically\n",
        "    * look for the nearest neighbor\n",
        "    * take the average\n",
        "    * insert random values\n",
        "    * create ML algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PXqvB7RyhiPw",
      "metadata": {
        "id": "PXqvB7RyhiPw"
      },
      "outputs": [],
      "source": [
        "threshold = 0.2 #to jest procent danych ktorych brakuje\n",
        "\n",
        "chipotle.columns[chipotle.isnull().mean() > threshold]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tF2JbNTXkzXf",
      "metadata": {
        "id": "tF2JbNTXkzXf"
      },
      "outputs": [],
      "source": [
        "# drop columns with more than 20% of missing data pozbywamy sie wgl kolumny gdzie brak danych jest wiekszy niz 20%\n",
        "chipotle[chipotle.columns[chipotle.isnull().mean() <= threshold]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MK_nkLVxk_dL",
      "metadata": {
        "id": "MK_nkLVxk_dL"
      },
      "outputs": [],
      "source": [
        "# drop rows with at least 20% of missing data (here one value) mniej hardcorowe podejscie - pozbywamy sie tylko wierszy gdzie w kolumnie sa braki danych, ale pozbywamy sie tez wartosci w pozostalych kolumnach gdzie choice description ma braki. wiec trezba zdecydowac czy faktycznie az tak nic nam nie mowia\n",
        "chipotle.loc[chipotle.isnull().mean(axis=1) < threshold] # wyrzucam wiersze wszystkie gdzie sa w ktorejkolwiek kolumnie braki wieksze niz 20%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artificially, generate more missing values:"
      ],
      "metadata": {
        "id": "6_oj4E2RsaUw"
      },
      "id": "6_oj4E2RsaUw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xce5DfrLlTb2",
      "metadata": {
        "id": "Xce5DfrLlTb2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "chipotle['item_price [$]'] = chipotle['item_price [$]'].replace(8.75, np.NaN) # tutaj dla danej item price zastepujemy wartosc brakiem danej NaN - ale to tylko dla cwiczenia bo nielogiczne generalnie zeby dane wywalac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oYzL6hdDltsh",
      "metadata": {
        "id": "oYzL6hdDltsh"
      },
      "outputs": [],
      "source": [
        "chipotle.isna().sum() #no i tu widzimy ze jest az 730 item price gdzie brak daych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5UHLAL74lyWK",
      "metadata": {
        "id": "5UHLAL74lyWK"
      },
      "outputs": [],
      "source": [
        "chipotle.fillna(0) #wszystkie braki danych zastepujemy 0 takze ponizej opiuje i zastepuje tylko itemprice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle[\"item_price [$]\"].fillna(0)"
      ],
      "metadata": {
        "id": "5-8cRYkP6pu4"
      },
      "id": "5-8cRYkP6pu4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bXjEhJAdl0Dy",
      "metadata": {
        "id": "bXjEhJAdl0Dy"
      },
      "outputs": [],
      "source": [
        "chipotle.fillna(chipotle['item_price [$]'].median()) #tu wszystkie braki danych zastepujemy za pomoca mediany zmiennej item price"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle['choice_description'].fillna(chipotle['item_price [$]'].median())"
      ],
      "metadata": {
        "id": "zjTIn-e97a7k"
      },
      "id": "zjTIn-e97a7k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3tQh7ijil7VS",
      "metadata": {
        "id": "3tQh7ijil7VS"
      },
      "outputs": [],
      "source": [
        "# missing categorical features replaced the most frequent category\n",
        "chipotle.fillna(chipotle['choice_description'].value_counts().idxmax())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle['choice_description'].fillna(chipotle['choice_description'].value_counts().idxmax())"
      ],
      "metadata": {
        "id": "ltoesZP_7j85"
      },
      "id": "ltoesZP_7j85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXKx-tnz7pV8"
      },
      "id": "FXKx-tnz7pV8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1MzdvyzJmaiu",
      "metadata": {
        "id": "1MzdvyzJmaiu"
      },
      "source": [
        "## Data preprocessing / cleaning (noise removal) odstające danych. widac ze najczesciej przyjmuje wartosci 8-12 ale nagle jest jakas wartosc bardzo odchylona co moze bardzo zaburzac analize, korelacje moze zmienic i zaburza caly porces. JEsli tych obserwacji jest duzo to nie mozna stwierdzic ze zakloca trend ale jest jest jak na histogramie to najlepiej uptliera sie pozbyc, po sprawdzeniu czy jest napewno outlierem\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=19yZ4fZuosFbjGLp_yCRS0mSKBSFv0v0Z\" alt=\"Data processing: outliers\" title=\"Data processing: outliers\" align=\"left\" width=\"600px\" hspace=\"20px\" vspace=\"20px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V-juinmynOQO",
      "metadata": {
        "id": "V-juinmynOQO"
      },
      "source": [
        "Remove outliers with standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pZ3KPj2tnLfz",
      "metadata": {
        "id": "pZ3KPj2tnLfz"
      },
      "outputs": [],
      "source": [
        "#reguła 3 sigm(3 odchyleń standardowych) do sprawdzenia czy napewno jest outlierem -> zakladajac ze nasza zmienna ma rozklad normalny lub do niego zblizony to jesli przesuway sie od sredniej na prawo i lewo to 66% obserwacij powinno spelniac to odchylenie, o 2 sigmy 30pare, a 3 sigmy w prawo i lewo od sredniej to 99% wartosci. jesli cos jest poza tym przedzialem to na 99% jest to wartsc odstająca.\n",
        "# dropping outlier observations (ie. rows) based on standard deviation\n",
        "#3 odchylenia\n",
        "factor = 3\n",
        "\n",
        "item_price_mean = chipotle['item_price [$]'].mean()\n",
        "item_price_std = chipotle['item_price [$]'].std()\n",
        "\n",
        "upper_lim = item_price_mean + item_price_std * factor\n",
        "lower_lim = item_price_mean - item_price_std * factor\n",
        "\n",
        "#mozemy dodac tez warunek ze brak danych zostaje\n",
        "mask = (chipotle['item_price [$]'] < upper_lim) & (chipotle['item_price [$]'] > lower_lim) | (chipotle['item_price [$]'] > lower_lim)\n",
        "\n",
        "\n",
        "chipotle[mask]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "item_price_std"
      ],
      "metadata": {
        "id": "GUWPH1Bo-Q7J"
      },
      "id": "GUWPH1Bo-Q7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TYIOHky6oKA4",
      "metadata": {
        "id": "TYIOHky6oKA4"
      },
      "outputs": [],
      "source": [
        "# pay attention to missing data!\n",
        "chipotle[~mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AHJUtkO5oStN",
      "metadata": {
        "id": "AHJUtkO5oStN"
      },
      "source": [
        "Removing outliers with percentiles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OoQbcqbHoSLn",
      "metadata": {
        "id": "OoQbcqbHoSLn"
      },
      "outputs": [],
      "source": [
        "upper_lim = chipotle['item_price [$]'].quantile(.95)\n",
        "lower_lim = chipotle['item_price [$]'].quantile(.05)\n",
        "\n",
        "chipotle[(chipotle['item_price [$]'] < upper_lim) & (chipotle['item_price [$]'] > lower_lim)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-PmRLpayouSK",
      "metadata": {
        "id": "-PmRLpayouSK"
      },
      "source": [
        "Limiting outliers instead of deleting - esp. important for small datasets (for large datasets, removing redundant information is not a problem)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aRC4zlp5o1R8",
      "metadata": {
        "id": "aRC4zlp5o1R8"
      },
      "outputs": [],
      "source": [
        "# w tym przypadku zawsze bedziemy mieli outliery. z tym rozwiazaniem ostrozeni, bo przewazeni usuwamy duzo wiecej obserwacji\n",
        "upper_lim = chipotle['item_price [$]'].quantile(.95)\n",
        "lower_lim = chipotle['item_price [$]'].quantile(.05)\n",
        "\n",
        "chipotle.loc[(chipotle['item_price [$]'] > upper_lim), 'item_price [$]'] = upper_lim #zamiastusuwania outlierow to mowimy ze to co nim jest to ustawiam jego wartosc jako wartość maxymalna czyli upperlim (gdy przekracza uppr_im czyli granice, lub wartos minimalna gdy jest ponizej lower_lim)\n",
        "chipotle.loc[(chipotle['item_price [$]'] < lower_lim), 'item_price [$]'] = lower_lim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upper_lim #podejscie bardziej konserwatywne"
      ],
      "metadata": {
        "id": "iUk_t0pn__my"
      },
      "id": "iUk_t0pn__my",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_lim"
      ],
      "metadata": {
        "id": "4NLB2sIXADS-"
      },
      "id": "4NLB2sIXADS-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle['item_price [$]'].value_counts()"
      ],
      "metadata": {
        "id": "fH1C4kJmAOPg"
      },
      "id": "fH1C4kJmAOPg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "xDAZ9TRJpl1V",
      "metadata": {
        "id": "xDAZ9TRJpl1V"
      },
      "source": [
        "### Standardization\n",
        "\n",
        "Standardization consists in standardizing the various data formats in a set:\n",
        "* language coding style: UTF-8, ANSI etc.,\n",
        "* Dates: Sep 12, 2019, Sep 12, 2019, 120919 etc.,\n",
        "* New York City, NY, Z. New York,\n",
        "* MIT, Massachusetts Institute of Technology\n",
        "* Unrecognizable signs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T9HKaG44pz_P",
      "metadata": {
        "id": "T9HKaG44pz_P"
      },
      "source": [
        "### Discretization\n",
        "np mamy analize krajow i chcemy to rozbic tylko na kontynenty wiec kategoryzujemy, hiszpanie i wlochy wrzucamy do Europy a chile i brazylie do ameryki pld - uogolnianiae, zmniejszamy zmiennosc zmiennej w przypadku wartosci\n",
        "\n",
        "Continuous data distribution is bad for most machine learning algorithms.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1O8fuKpYz3hR2Ab6d7SOUZLrvQWqDzM55\" alt=\"Data processing: discretization\" title=\"Data processing: discretization\" align=\"right\" width=\"300px\" hspace=\"20px\" vspace=\"20px\"/>\n",
        "\n",
        "> numerical binning example\n",
        "\n",
        "| Value  | Bin  |\n",
        "| ------ | ---- |\n",
        "| 0-30   | Low  |\n",
        "| 31-70  | Mid  |\n",
        "| 71-100 | High |\n",
        "\n",
        "> categorical binning example\n",
        "\n",
        "| Value  | Bin  |\n",
        "| ------ | ---- |\n",
        "| Spain  | Europe |\n",
        "| Italy  | Europe |\n",
        "| Chile  | South America |\n",
        "| Brazil | South America |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A1xqKObmrj5C",
      "metadata": {
        "id": "A1xqKObmrj5C"
      },
      "source": [
        "### Logarithmic transformation\n",
        "\n",
        "* helps to deal with skewed data\n",
        "* organizes the size of the data\n",
        "* reduces the impact of outliers\n",
        "* increases the reliability of the model\n",
        "* negative values need to be dealt with\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pHrjlHcQplRp",
      "metadata": {
        "id": "pHrjlHcQplRp"
      },
      "outputs": [],
      "source": [
        "(chipotle['item_price [$]'] + 1).transform(np.log)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value =-5\n",
        "value-value+1"
      ],
      "metadata": {
        "id": "dAwfAHWhIGEi"
      },
      "id": "dAwfAHWhIGEi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle['item_price [$]'].min()"
      ],
      "metadata": {
        "id": "4edtJyvDILar"
      },
      "id": "4edtJyvDILar",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HSQSR1RIr7vh",
      "metadata": {
        "id": "HSQSR1RIr7vh"
      },
      "outputs": [],
      "source": [
        "# if there are negative values present - jesli mamy orblem z asymetria danych to za pomoca logarytmu mozemy przejsc na liniowe i jest duzo lepsza interpretacja np przechodzimy od razu na interpetacje procentowa. lepsze bo wartosci nominalne sa czesto przeklamane. np w przypadku zarobkow w zal odinflacji etc wartosc nominalna znaczyc moze co innego, a procentowe wyrazenie zawsze odzwierciedla\n",
        "(chipotle['item_price [$]'] - chipotle['item_price [$]'].min() + 1).transform(np.log)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1UlweCzwsGrT",
      "metadata": {
        "id": "1UlweCzwsGrT"
      },
      "source": [
        "### One-hot encoding (dummy variables) - zamienianie wartosci tekstowej na osobne kolumny . tworzymy sobie dodatkowe kolumny i dlaczego nie mamy kolumny dla ROme? bo jesli istambul i madryt jest 0 to wiadomo ze musi to byc Roma. wiec nie potrzebujemy miec az tylu wymiarow\n",
        "\n",
        "| User   | City     | → | User | Istanbul | Madrid |\n",
        "| ------ | ----     | --- | --- | --- | --- |\n",
        "| 1      | Roma     | → | 1 | 0 | 0 |\n",
        "| 2      | Madrid   | → | 2 | 0 | 1 |\n",
        "| 1      | Madrid   | → | 1 | 0 | 1 |\n",
        "| 3      | Istanbul | → | 3 | 1 | 0 |\n",
        "| 2      | Istanbul | → | 2 | 1 | 0 |\n",
        "| 1      | Istanbul | → | 1 | 1 | 0 |\n",
        "| 1      | Roma     | → | 1 | 0 | 0 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xg6XLaPBto8u",
      "metadata": {
        "id": "Xg6XLaPBto8u"
      },
      "source": [
        "### GroupBy & apply\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=17TC0l9R_fE2-d932J6YILBCUu8pj_ox7\" alt=\"Data processing: groupby\" title=\"Data processing: groupby\" align=\"left\" width=\"600px\" hspace=\"20px\" vspace=\"20px\"/>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle.item_name.unique()"
      ],
      "metadata": {
        "id": "PeWWp12TJggi"
      },
      "id": "PeWWp12TJggi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle.head(10) # za duzo wiec wezmy pierwsze 10 obserwacji"
      ],
      "metadata": {
        "id": "g3khcUt6JmO7"
      },
      "id": "g3khcUt6JmO7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle_10=chipotle.head(10)"
      ],
      "metadata": {
        "id": "GNH7aIAGJtk-"
      },
      "id": "GNH7aIAGJtk-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(chipotle_10['item_name']).astype(int) #(zeby zamiast false/true dawalo 0/1)"
      ],
      "metadata": {
        "id": "m0HczuHmJyLQ"
      },
      "id": "m0HczuHmJyLQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "powyzej nie widac zeby ktorakolwiek kolumna miala same zera -> opcja dropfirst jest i wtedy mozemy wyrzucic kolumne ktorakolwiek, bo domyslnie bedziemy wiedzieli ze tam gdzie sa same 0 to ta kolumna ktorej nie ma."
      ],
      "metadata": {
        "id": "X6IMVorWKGIm"
      },
      "id": "X6IMVorWKGIm"
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(chipotle_10['item_name'], drop_first=True).astype(int)"
      ],
      "metadata": {
        "id": "hQfRBwp_KatR"
      },
      "id": "hQfRBwp_KatR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4Wc6yhzDt4oP",
      "metadata": {
        "id": "4Wc6yhzDt4oP"
      },
      "outputs": [],
      "source": [
        "chipotle.groupby('item_name').sum()[['quantity', 'item_price [$]']] # grupujemy i sumujemy sobie wszstkie wystapienia tych itemnamow i zliczamy quantity ile jest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle.groupby('item_name').sum()[['quantity', 'item_price [$]']] #dajemy wiecej niz jedna kolumne wiec dajemy zbior zmiennych, wiec gdybysmy zrobili tylko 1 nawias kwadratowy to by bylo nie jako zbior danych tylko pojedyncze wartosci. jesli mamy dwie kolumny to musi byc jako zbior danych. grupowanie juz bylo na zajeciach z Pandasa"
      ],
      "metadata": {
        "id": "0qxWaxrdK68B"
      },
      "id": "0qxWaxrdK68B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "IXJMi0jtuPnT",
      "metadata": {
        "id": "IXJMi0jtuPnT"
      },
      "source": [
        "### Split text\n",
        "\n",
        "Useful when you have multiple traits in one column."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle['item_name'] # tu widzimy ze tekst podzielony spacjami"
      ],
      "metadata": {
        "id": "XgTI67LvLcyj"
      },
      "id": "XgTI67LvLcyj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle['item_name'].str.split(' ') #wiec pierwsza rzecz ktora robimy to jak w kodzie nizej -> str ze tekst a potem split czyli podziel i na podst czego - w cudzyslow wrzucamy spacje"
      ],
      "metadata": {
        "id": "wZl6VuPgLzX3"
      },
      "id": "wZl6VuPgLzX3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JQJzLXjbuuyo",
      "metadata": {
        "id": "JQJzLXjbuuyo"
      },
      "outputs": [],
      "source": [
        "chipotle['item_name'].str.split(' ').map(lambda x: x[0]) # gdy np chcemy z tekstu wydobyc tylko pierwszy człon - uzywamy funkcji indeksowania, wydobacz pierwszy element"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chipotle[chipotle['item_name'].str.startswith('Chicken')]['item_name'] #funkcja startswith od tego zeby zacelo nam od jkaiegos slowa"
      ],
      "metadata": {
        "id": "c0BL8E-oMS3v"
      },
      "id": "c0BL8E-oMS3v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J4xNnFFJuT1S",
      "metadata": {
        "id": "J4xNnFFJuT1S"
      },
      "outputs": [],
      "source": [
        "chipotle[chipotle['item_name'].str.startswith('Chicken')]['item_name'].value_counts() #bardziej wyrafinowane podjecie - wez tylko te obiekty gdzie zaczyna sie od chicken i jeszcze pokaz ile kazdego jest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L3o18vtLu45u",
      "metadata": {
        "id": "L3o18vtLu45u"
      },
      "source": [
        "### Scaling - skalowanie wartosci - czyli wracamy do tematu wartosci ciaglych\n",
        "\n",
        "In most cases, the numerical characteristics of a dataset are not specific and vary from one another. In fact, it doesn't make sense to expect the age and income columns to have the same range. But from a machine learning perspective, the same scope helps improve the model.\n",
        "\n",
        "Scaling solves this problem. Continuous functions become range identical after the scaling process. This process is not mandatory for many algorithms, but still worthwhile. However, distance-based algorithms such as k-NN or k-Means must have scaled continuous functions as model inputs.\n",
        "Basically, there are two common ways to scale: normalization and standardization.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oCDm1SR6u-vJ",
      "metadata": {
        "id": "oCDm1SR6u-vJ"
      },
      "outputs": [],
      "source": [
        "chipotle['item_price [$]'].describe() #w niektorych algorytmach jest tak wymagane ze trezba wartosci wystandaryzowac zeby byly z jakiegos zakresu wartosci, albo zeby sie zachowywaly w jakis sposob, bo tylko wtedy algorytm bedzie dobrze dzialaal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OxwmYkBBvEJD",
      "metadata": {
        "id": "OxwmYkBBvEJD"
      },
      "outputs": [],
      "source": [
        "# normalization - od kazdej wartosci odejmujemy wartosc minimalna a nast dzielimy przez roznice pom wartoscia max a minimalna. wzor na slacku na normalizacje\n",
        "#doprowadzamy do sytuacji gdzie wszystkie wartosci sa z zakresu 0-1 czyli operujemy na % w sumie. to tez sposob na pozbycie sie wartosci odstajacych, ale lepiej najpierw sie tych odstajacych ozbyc bo inaczej itak bedzie nam zaburzac.\n",
        "\n",
        "item_price_normalized = (chipotle['item_price [$]'] - chipotle['item_price [$]'].min()) / \\\n",
        "(chipotle['item_price [$]'].max() - chipotle['item_price [$]'].min())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#kolejny sposob sklaowania wartosci to standaryzacja - wzor na standaryzacje jest na slacku\n",
        "#x czyli wartosc, odejmujemy od tego srednia wartosc i dzielimy przez odchylenie standardowe"
      ],
      "metadata": {
        "id": "nPgWVB6jN9Z2"
      },
      "id": "nPgWVB6jN9Z2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y7C5J1avvRig",
      "metadata": {
        "id": "Y7C5J1avvRig"
      },
      "outputs": [],
      "source": [
        "item_price_normalized.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7l2VR09va2n",
      "metadata": {
        "id": "e7l2VR09va2n"
      },
      "outputs": [],
      "source": [
        "# standardization\n",
        "\n",
        "item_price_standardized = (chipotle['item_price [$]'] - chipotle['item_price [$]'].mean()) / \\\n",
        "chipotle['item_price [$]'].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZVPha7Leviek",
      "metadata": {
        "id": "ZVPha7Leviek"
      },
      "outputs": [],
      "source": [
        "item_price_standardized.describe() #tu mamy wartosci sprowadozne do rozkladu normalnego i mamy srednia bardzo blisko zeru. jest trohce asymetria lewostornna bo wartosc minimalna bezwgl. jest wieksze niz 1,22 max. mamy wystandaryzowany rozklad, zbierzny z rozkładem normalnym dzieki standaryzacji\n",
        "#standaryzjaca wymagana czesto w wielu algorytmach bo radza sobie lepiej z wartosciami wystand. bo działamy na wartosciach z podobnych rozkladow. czyli rozne wartosci z roznych zmiennych ale z podobnych rozkladów"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QwFLhDpGvpJm",
      "metadata": {
        "id": "QwFLhDpGvpJm"
      },
      "source": [
        "### Data extraction - wydobywanie dat, ekstrakcja dat. dosc wazne, czasem upierdliwe, trzeba pokombinowac, bo bardzo czesto mamy np dane rozlozone w czasie np dane giełdowe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZTcX6sRbvr_N",
      "metadata": {
        "id": "ZTcX6sRbvr_N"
      },
      "outputs": [],
      "source": [
        "from datetime import date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZTYIwUi8vtqz",
      "metadata": {
        "id": "ZTYIwUi8vtqz"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame({\n",
        "    'date': ['01-01-2017', '04-12-2008', '23-06-1988', '25-08-1999', '20-02-1993']\n",
        "})\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "flZomjlUwCqK",
      "metadata": {
        "id": "flZomjlUwCqK"
      },
      "outputs": [],
      "source": [
        "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\") #zamienia nam konkretna kolumne ze zbioru danych date i zamienia nam te kolumne na typ Data. dodatkowo wczytujemy sobie format czyli mowimy ze pierwsze to dzien-%m miesiac-%Y rok z duzej litery  a małe y to format 24\n",
        "#Y - format 2024\n",
        "#y - format 24\n",
        "# gdyby byl czas to najczesniej %H (czyli hours itd):%M:%S dwukropek zazwyczaj ale zwrocic uwage jak jest rozdzielony"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "1XF_F_rJQwtG",
        "outputId": "5eaa2f4c-1883-4587-f31d-25aac548de14"
      },
      "id": "1XF_F_rJQwtG",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6226a73926db>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v1UAXlsbwP07",
      "metadata": {
        "id": "v1UAXlsbwP07"
      },
      "outputs": [],
      "source": [
        "data['date'].dt.year # tu mozemy z daty wydobyc tylko rok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wrsRNxuowSzj",
      "metadata": {
        "id": "wrsRNxuowSzj"
      },
      "outputs": [],
      "source": [
        "data['date'].dt.month #albo wydobyc sobie tylko. dzien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hwOAxKzrwUOY",
      "metadata": {
        "id": "hwOAxKzrwUOY"
      },
      "outputs": [],
      "source": [
        "data['date'].dt.day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fYne1U6wWDn",
      "metadata": {
        "id": "3fYne1U6wWDn"
      },
      "outputs": [],
      "source": [
        "data['date'].dt.day_of_week #wydobycie info jaki to dzien tygodnia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uCI7oEQqwYSh",
      "metadata": {
        "id": "uCI7oEQqwYSh"
      },
      "outputs": [],
      "source": [
        "data['passed_years'] = date.today().year - data['date'].dt.year\n",
        "data['passed_years']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dspschTUwgV4",
      "metadata": {
        "id": "dspschTUwgV4"
      },
      "outputs": [],
      "source": [
        "data['passed_months'] = 12 * data['passed_years'] + date.today().month - data['date'].dt.month #ile minelo miesiecy\n",
        "data['passed_months']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8_dFE5wwryg",
      "metadata": {
        "id": "f8_dFE5wwryg"
      },
      "outputs": [],
      "source": [
        "data['day_name'] = data['date'].dt.day_name() # no i wydobycie nazwy dnia jaki byl wtedy\n",
        "data['day_name']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0vMHSeZw1w8",
      "metadata": {
        "id": "d0vMHSeZw1w8"
      },
      "source": [
        "### Feature engineering example - czyli wydobywanie, tworzenie nowych zmiennych z danych surowych\n",
        "\n",
        "How to assess whether a given profile on Facebook is not a bot? - np chcemy sprawdzic czy dany profil jest botem czy nie, bo wtedy fb nic na tym nie zarabia bo generuje ruch ale wiadomo ze niczego nie kupi\n",
        "\n",
        "* number of friends\n",
        "* number of posts published\n",
        "* number of photos\n",
        "* number of likes\n",
        "* likes under posts\n",
        "* the length of the posts\n",
        "* the origin of the phone number - bo sa kraje gdzie tych botow jest wiecej\n",
        "* e-mail address domainbot nie dzialal po 5-6 iteracjach\n",
        "* jak rusza myszka po ekranie czyli np czy uzytkownik skrolujac info rusza kursorem\n",
        "*bot chodzacy postronie i zbierajacy dane o nich-> ustawic sie na stronie i przejechac do konca i wtedy bot byl w stanie zauwazyc wsystkie ogloszenia, ale po 2 latach juz\n",
        "\n",
        "And the rating of the attractiveness of a movie on YouTube?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GgTU1sTcw-MS",
      "metadata": {
        "id": "GgTU1sTcw-MS"
      },
      "source": [
        "The car rental company ARRA wants to evaluate the reliability and maintenance costs of the different car models available in the fleet. It has the following data:\n",
        "* a table with individual activities of all cars in the fleet: date, place of rental, status (driving, breakdown), renting data (age, sex, nationality), number of kilometers traveled, amount of fuel consumed, car identification number\n",
        "* a table for each car: year of production, engine, equipment, brand, model, etc.\n",
        "\n",
        "\n",
        "What features can we find here?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WKJg3v6pyp1P",
      "metadata": {
        "id": "WKJg3v6pyp1P"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1UO2urRciECzoKE_vHy4RMGfFbkOWOGlW\" alt=\"SDA logo\" align=\"left\" width=\"100px\" hspace=\"10px\" vspace=\"10px\"/>\n",
        "<br>\n",
        "\n",
        "# TASKS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<font color='#306998'>Information for TASKs </font><font color='#ffd33b'>1-2</font>**\n",
        "\n",
        "The following data was was made available on https://www.kaggle.com/datasets/davidbnn92/weather-data-for-covid19-data-analysis/.\n",
        "\n",
        "Try downloading it from there using the `Download` button or with the API command `!kaggle datasets download -d davidbnn92/weather-data-for-covid19-data-analysis` (see https://www.kaggle.com/discussions/general/74235 for more details).\n",
        "\n",
        "If the data is no longer available you can always download it from our Google Drive https://drive.google.com/drive/folders/1KXr6yUW7rE0LzUzuuMehEzam8D9zxGpv?usp=sharing.\n",
        "\n",
        "### About Dataset\n",
        "\n",
        "The dataset contains selected metereological features, such as temperature or wind speed, and was imported from the `NOAA GSOD dataset`, continuously updated to include recent measurments.\n",
        "\n",
        "> Among others, you can find the following columns here:\n",
        "* `Id`\n",
        "* `Country/Region`\n",
        "* `Date`\n",
        "* `temp`: Mean temperature for the day in degrees Fahrenheit to tenths.\n",
        "* `max`: Maximum temperature reported during the day.\n",
        "* `min`: Minimum temperature reported during the day.\n",
        "* `stp`: Mean station pressure for the day in millibars to tenths.\n",
        "* `slp`: Mean sea level pressure for the day.\n",
        "* `dewp`: Mean dew point for the day in [Fahrenheit to tenths].\n",
        "* `wdsp`: Mean wind speed for the day in [knots to tenths].\n",
        "* `prcp`: Total precipitation (rain and/or melted snow) reported during the day in [inches and hundredths]; `.00` indicates no measurable precipitation (includes a trace).\n",
        "* `fog`: Indicators (1 = yes, 0 = no/not reported) for the occurrence during the day.</br></br>\n",
        "Note that time of max/min temperatures varies by country and region, so this will sometimes not be the max for the calendar day."
      ],
      "metadata": {
        "id": "C4QRRBMdBUTD"
      },
      "id": "C4QRRBMdBUTD"
    },
    {
      "cell_type": "markdown",
      "id": "N8hrKaJaecEc",
      "metadata": {
        "id": "N8hrKaJaecEc"
      },
      "source": [
        "## **<font color='#306998'>TASK </font><font color='#ffd33b'>1</font>**\n",
        "\n",
        "Check how many rows contain missing data in `slp` column. Delete those rows and reset the rows indexes (see the `.reset_index()` method).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_weather=pd.read_csv('/content/training_data_with_weather_info_week_4 (1).csv')\n",
        "df_weather=df_weather[['Id', 'Country_Region', 'Date',\n",
        "       'temp', 'min', 'max', 'stp', 'slp', 'dewp', 'wdsp', 'prcp',\n",
        "       'fog']] #chce miec tylko te kolumny id itd ktore mnie interesuja to przekopiowane z wczesiejszego zadnaia i tu  wzielam z czatu"
      ],
      "metadata": {
        "id": "LlKpktAztDS-"
      },
      "id": "LlKpktAztDS-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. sprawdz ile wierszy ma braki danych w kolumnie spl a najpierw wgl mozesz sprawdzic ile jest w calym zbiorze dla kazdej kolumeny\n",
        "#2. usun wiersze gdzie slp ma brak danych\n",
        "#3. zresetuj indeksy czyli zeby jak usuniemy wiersze to beda wyrywki wiec ma ladnie wygladac czyli od 0 do wartosci max bez zadnych dziur - to bylo na pandasie o resetowaniu indexow metoda reset_index()method"
      ],
      "metadata": {
        "id": "lbBdukdiVlsq"
      },
      "id": "lbBdukdiVlsq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sprawdzam ile wgl mam danych\n",
        "len(df_weather)"
      ],
      "metadata": {
        "id": "FoqCkOZRWlNH"
      },
      "id": "FoqCkOZRWlNH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sprawdzam ile mam braków w wierszach w kolumnie spl\n",
        "import numpy as np\n",
        "df_weather.isna().sum()"
      ],
      "metadata": {
        "id": "QXDu8aTrW3gw"
      },
      "id": "QXDu8aTrW3gw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather.isna().sum()/len(df_weather) #przechodzimy na procenty"
      ],
      "metadata": {
        "id": "QWQcP6R3YuH8"
      },
      "id": "QWQcP6R3YuH8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#usun wiersze gdzie slp ma brak danych\n",
        "df_weather=df_weather[~df_weather['slp'].isnull()].reset_index()"
      ],
      "metadata": {
        "id": "DhW99FJZYCY-"
      },
      "id": "DhW99FJZYCY-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HudTJvrnG3l_"
      },
      "id": "HudTJvrnG3l_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#usuwam kolumne z indexem"
      ],
      "metadata": {
        "id": "-HVpqXOmZDeW"
      },
      "id": "-HVpqXOmZDeW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_VKtl1NG4Et"
      },
      "id": "5_VKtl1NG4Et",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "gbcUVDw-gqoG",
      "metadata": {
        "id": "gbcUVDw-gqoG"
      },
      "source": [
        "## **<font color='#306998'>TASK </font><font color='#ffd33b'>2</font>**\n",
        "\n",
        "For the data frame from **Task 1**:\n",
        "* generate missing data by removing the randomly selected 10% of values and replacing them with `-99` (e.g. use the following command `df_weather = df_weather.mask(np.random.random(df.shape) < .1)`)\n",
        "* fill in the missing data with the\n",
        "    * mean value\n",
        "    * median value\n",
        "    * mean value wrt. to Country (groupby first to get mean values)\n",
        "* remove outlier observations\n",
        "* enter a new column with discrete values for temperature (you can use the `cut()` or `qcut()` methods from the `pandas` library)\n",
        "* enter the so-called *dummy variables* for those discrete temperature values\n",
        "* use normalization or standardization to scale the temperature data\n",
        "* create an additional `date` column containing the date in `pd.DateTime()` format\n",
        "\n",
        "Tips:\n",
        "* the missing data in the form `-99` can be replaced with something standard, e.g. `df_weather = df_weather.replace(-99, np.NaN)` then all standard methods such as: `.isna(), .isnull(), .fillna(), .dropna()` can be used\n",
        "* to generate dummy variables, the so-called one-hot-encoding can be used:\n",
        "    * `pd.get_dummies()` from pandas\n",
        "    * `sklearn.preprocessing.OneHotEncoder` from scikit-learn\n",
        "    * `tf.keras.layers.CategoryEncoding` from tensorflow\n",
        "* data scaling can be done as in the slides or with the scikit-learn library, such as `sklearn.preprocessing.StandardScaler` or `sklearn.preprocessing.MinMaxScaler`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather['temp'].fillna(df_weather['Country_Region'].map(df_weather.groupby('Country_Region')['temp'].mean()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "t3IwjW5TLSpR",
        "outputId": "c713dee3-670c-4075-db2e-1ee74ff1170f"
      },
      "id": "t3IwjW5TLSpR",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_weather' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b318808d74b3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_weather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_weather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Country_Region'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_weather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Country_Region'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_weather' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#utworzenie wykresu\n",
        "\n",
        "import seaborn as sns\n",
        "ax=sns.histplot(data=df_weather,x='temp')\n",
        "ax.axvline(df_weather['temp'].mean(),color='k',lw=2)\n",
        "ax.axvline(df_weather['temp'].mean()-df_weather['temp'].std()*3,color='r',lw=1,ls='--')\n",
        "ax.axvline(df_weather['temp'].mean()+df_weather['temp'].std()*3,color='r',lw=1,ls='--')\n",
        "factor=3\n",
        "temp_mean=df_weather['temp'].mean()\n",
        "temp_std=df_weather['temp'].std()\n",
        "\n",
        "upper_lim=temp_mean+temp_std*factor\n",
        "lower_lim=temp_mean-temp_std*factor\n",
        "\n",
        "mask=((df_weather['temp']<upper_lim) & (df_weather['temp']>lower_lim)) | (df_weather['temp'].isnull())"
      ],
      "metadata": {
        "id": "1jVeTTF7tMfh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "4fc8c783-e740-42ce-f12a-3cf48635326f"
      },
      "id": "1jVeTTF7tMfh",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_weather' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-cad9e293c480>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_weather\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_weather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_weather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdf_weather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_weather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdf_weather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_weather' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zdefiniuj ile ma byc binsow\n",
        "pd.cut(df_weather['temp'],bins=10).value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "2yvrX9ksOn9F",
        "outputId": "edd1783d-d286-485d-dd84-dc14cd5104f9"
      },
      "id": "2yvrX9ksOn9F",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_weather' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ca00b3be1411>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#zdefiniuj ile ma byc binsow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_weather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tmep'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_weather' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.qcut(df_weather['temp'],bins=10).value_counts() #qcut dzieli nam na decyle jesli damy 10"
      ],
      "metadata": {
        "id": "pVfAFFqWO5Pm"
      },
      "id": "pVfAFFqWO5Pm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as numpy"
      ],
      "metadata": {
        "id": "U7QnuiimON7N"
      },
      "id": "U7QnuiimON7N",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rW3QuULOORsZ"
      },
      "id": "rW3QuULOORsZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}